{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import datetime\n",
    "\n",
    "class Panorama:\n",
    "\n",
    "    # the blend types\n",
    "    NO_BLEND = 1\n",
    "    BOTH = 2\n",
    "    @classmethod\n",
    "    def padblack(cls, left, right, increase=2):\n",
    "        h = (left.shape[0] + right.shape[0])*increase\n",
    "        w = (right.shape[1]+left.shape[1])*increase\n",
    "        leftpadded = np.zeros([h, w ,3],dtype=np.uint8)\n",
    "        rightpadded = np.zeros([h, w ,3],dtype=np.uint8)\n",
    "        leftbeg = (w-left.shape[1])//2\n",
    "        upbeg = (h-left.shape[0])//2\n",
    "        leftpadded[upbeg:upbeg+left.shape[0], leftbeg:leftbeg+left.shape[1]] = left\n",
    "        rightpadded[upbeg:upbeg+right.shape[0], leftbeg:leftbeg+right.shape[1]] = right\n",
    "        return (leftpadded, rightpadded)\n",
    "        \n",
    "    # we make the weighed average of the two images\n",
    "    # the idea roughly comes from here: http://bigwww.epfl.ch/publications/thevenaz0701.pdf\n",
    "    # the above link found here: https://stackoverflow.com/questions/36386968/image-stitching-methods-to-remove-seams-for-stitched-image\n",
    "    @classmethod\n",
    "    def blendWeighedOld(cls, imgA, imgB, thresh=3):\n",
    "        result = np.zeros([imgA.shape[0], imgA.shape[1] ,3], dtype=np.uint8)\n",
    "        distA = cv2.distanceTransform(cv2.cvtColor(imgA, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "        distB = cv2.distanceTransform(cv2.cvtColor(imgB, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "        #cv2.imwrite('distA.png', distA)\n",
    "        #cv2.imwrite('distB.png', distB)\n",
    "        # for every pixel within the result image (that has the same dimensions as the imgA/imgB!)\n",
    "        for i in range(imgA.shape[0]):\n",
    "            for j in range(imgA.shape[1]):\n",
    "                # if the current pixel is \"out of\" the first image (or is just black)\n",
    "                if np.all(imgA[i,j] < thresh):\n",
    "                    result[i,j] = imgB[i,j]\n",
    "                # if the current pixel is \"out of\" the second image (or is just black)\n",
    "                elif np.all(imgB[i,j] < thresh):\n",
    "                    result[i,j] = imgA[i,j]\n",
    "                # else, we make the weighted sum and do the magic\n",
    "                # notice that here distA[i,j]+distB[i,j] can't be zero, since\n",
    "                # the upper cases covered that\n",
    "                else:\n",
    "                    result[i,j] = (imgA[i,j]*distA[i,j] + imgB[i,j]*distB[i,j]) / (distA[i,j]+distB[i,j])\n",
    "        \n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # the new blending\n",
    "    @classmethod\n",
    "    def blendWeighed(cls, imgA, imgB, threshold=3):\n",
    "        distA = cv2.distanceTransform(cv2.cvtColor(imgA, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "        distB = cv2.distanceTransform(cv2.cvtColor(imgB, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "        result = np.zeros(imgA.shape)\n",
    "        # I don't know better atm but the dimensions of the distA, distB and the imgA and imgB are not fitting\n",
    "        # and I cannot use slicing if they don't fit\n",
    "        a = np.asarray(np.dstack((distA, distA, distA)), dtype=np.float32)\n",
    "        b = np.asarray(np.dstack((distB, distB, distB)), dtype=np.float32)\n",
    "        div = a + b\n",
    "        mask = (imgA >= threshold) & (imgB >= threshold)\n",
    "        result[mask] = (a*imgA+b*imgB)[mask]/div[mask]\n",
    "        result[imgA < threshold] = imgB[imgA < threshold]\n",
    "        result[imgB < threshold] = imgA[imgB < threshold]\n",
    "        return result\n",
    "   \n",
    "    # naive merge; seams are to be seen, probably\n",
    "    @classmethod\n",
    "    def merge(cls, imgDest, imgSrc):\n",
    "        # and just paste the second image onto the first one\n",
    "        result = np.copy(imgDest)\n",
    "        for i in range(imgSrc.shape[0]):\n",
    "            for j in range(imgSrc.shape[1]):\n",
    "                if(np.all(imgSrc[i,j] >= 10)):\n",
    "                    result[i,j] = imgSrc[i,j]\n",
    "        #result[0:imgSrc.shape[0], 0:imgSrc.shape[1]] = imgSrc\n",
    "        return result\n",
    "    \n",
    "    # obtaining the keypoints and their features for an image\n",
    "    @classmethod\n",
    "    def detectAndDescribe(cls, image):\n",
    "        # convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and extract features from the image\n",
    "        descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "        (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "        return (kps, features)\n",
    "    \n",
    "    # the method for drawing matches in the image left and right, given the \n",
    "    # keypoints and matches\n",
    "    @classmethod\n",
    "    def drawMatches(cls, right, kpsR, left, kpsL, matches):\n",
    "        matchImg = None\n",
    "        matchImg = cv2.drawMatches(right, kpsR, left, kpsL, matches, matchImg)\n",
    "        return matchImg\n",
    "    @classmethod\n",
    "    def drawKeypoints(cls, image, keypoints):\n",
    "        draw = None\n",
    "        draw = cv2.drawKeypoints(image, keypoints, draw)\n",
    "        return draw\n",
    "    \n",
    "    @classmethod\n",
    "    def transform(cls, matches, kpsR, kpsL, left, right):\n",
    "        matchesIdx = [(m.queryIdx, m.trainIdx) for m in matches[:10]]\n",
    "        matchesCoordR = np.float32([kpsR[mind[0]].pt for mind in matchesIdx])\n",
    "        matchesCoordL = np.float32([kpsL[mind[1]].pt for mind in matchesIdx])\n",
    "        (H, mask) = cv2.findHomography(matchesCoordR, matchesCoordL, cv2.RANSAC)\n",
    "        \n",
    "        imgDest = None\n",
    "        imgDest = cv2.warpPerspective(right, H, (right.shape[1], right.shape[0]))\n",
    "        \n",
    "        return imgDest\n",
    "        \n",
    "    @classmethod\n",
    "    def makePanorama(cls, right, left, drawKeypoints=False, drawMatches=False, blend_type=NO_BLEND):\n",
    "        # the result is a dictionary that contains all the possible images derived:\n",
    "        # result with no blending\n",
    "        # result with blending\n",
    "        # right image with keypoints drawn\n",
    "        # left image with keypoints drawn\n",
    "        # both images with matches drawn\n",
    "        result = {'resNoBlend':None, 'resBlend':None, 'kpsR':None, 'kpsL':None, 'matches':None}\n",
    "        beg = datetime.datetime.now()\n",
    "        \n",
    "        l, r = cls.padblack(left, right)\n",
    "        left, right = l, r\n",
    "        \n",
    "        # first, we find the keypoints in the images and their features \n",
    "        (kpsR, ftsR) = cls.detectAndDescribe(right)\n",
    "        (kpsL, ftsL) = cls.detectAndDescribe(left)\n",
    "        end = datetime.datetime.now()\n",
    "        print(\"Keypoint detection in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "        # if we should draw the keypoints, we draw them and put in the dictionary\n",
    "        if drawKeypoints:\n",
    "            result['kpsR'] = cls.drawKeypoints(right, kpsR)\n",
    "            result['kpsL'] = cls.drawKeypoints(left, kpsL)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #orb = cv2.ORB_create()\n",
    "        \n",
    "        # we find the matcher for the images and sort them by distance (by relevance)\n",
    "        beg = datetime.datetime.now()\n",
    "        \n",
    "        bf = cv2.BFMatcher(cv2.NORM_L1,crossCheck=False)\n",
    "        matches = bf.match(ftsR,ftsL)\n",
    "        matches = sorted(matches, key=lambda x:x.distance)\n",
    "        \n",
    "        end = datetime.datetime.now()\n",
    "        print(\"Matching in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "        \n",
    "        # if we should draw the matches, we draw them and put the result in the dictionary\n",
    "        if drawMatches:\n",
    "            result['matches'] = cls.drawMatches(right, kpsR, left, kpsL, matches[:10])\n",
    "        \n",
    "        beg = datetime.datetime.now()\n",
    "        \n",
    "        # we transform the right image\n",
    "        newDst = cls.transform(matches, kpsR, kpsL, left, right)\n",
    "        #cv2.imwrite('newDst.png', newDst)\n",
    "       \n",
    "        end = datetime.datetime.now()\n",
    "        print(\"Applying the transformation in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "    \n",
    "        # we pad the left image with black background so that the two images have matching dimensions\n",
    "        #newSrc = np.zeros([left.shape[0] + right.shape[0], right.shape[1]+left.shape[1] ,3],dtype=np.uint8)\n",
    "        #newSrc[0:left.shape[0], 0:left.shape[1]] = left\n",
    "        newSrc = left\n",
    "        #cv2.imwrite('newSrc.png', newSrc)\n",
    "        \n",
    "        # now, both images that we use are in newDst (right) and newSrc (left)\n",
    "        \n",
    "        # depending on the type of blending chosen, we blend (or don't) and put the results \n",
    "        # in the dictionary\n",
    "        # since the newSrc is now padded, we use left!\n",
    "        beg = datetime.datetime.now()\n",
    "        \n",
    "        tmp_no_blend = cls.mergeProto(newDst, newSrc)\n",
    "        cropped, (y, yh, x, xw) = cls.crop_rect(tmp_no_blend)\n",
    "        result['resNoBlend'] = cropped\n",
    "        newDst1 = newDst[y:yh, x:xw]\n",
    "        newSrc1 = newSrc[y:yh, x:xw]\n",
    "    \n",
    "        end = datetime.datetime.now()\n",
    "        print(\"Copying in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "        \n",
    "        if blend_type == cls.BOTH:\n",
    "            beg = datetime.datetime.now()\n",
    "            \n",
    "            result['resBlend'] = cls.blendWeighed(newDst1, newSrc1)\n",
    "            \n",
    "            end = datetime.datetime.now()\n",
    "            print(\"Blending in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "        \n",
    "        return result\n",
    "    # cropping doesn't remove ALL the black parts, just the maximal amount that\n",
    "    # keeps all the pixels of the stitched images \n",
    "    @classmethod\n",
    "    def crop(cls,img):\n",
    "        imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret, thresh = cv2.threshold(imgray, 10, 10, 10)\n",
    "        im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        maxa = 0\n",
    "        maxi = 0\n",
    "        for i in range(len(contours)):\n",
    "            tmpmax = cv2.contourArea(contours[i])\n",
    "            if maxa < tmpmax:\n",
    "                maxa = tmpmax\n",
    "                maxi = i\n",
    "\n",
    "        rect=cv2.minAreaRect(contours[maxi])\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "\n",
    "        x = box[1][0]\n",
    "        y = box[2][1]\n",
    "        w = box[0][0] - box[1][0]\n",
    "        h = box[1][1] - box[2][1]\n",
    "        result = img[y: y + h, x: x + w]\n",
    "        return result\n",
    "    @classmethod\n",
    "    def crop_new(cls,img):\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "        im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt = contours[0]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        cntarea = w*h\n",
    "        # not sure if already sorted so finding maximal area rectangle, that one's ours\n",
    "        for i in range(1, len(contours)):\n",
    "            x,y,w,h = cv2.boundingRect(contours[i])\n",
    "            newarea = w*h\n",
    "            if newarea > cntarea:\n",
    "                cnt = contours[i]\n",
    "                cntarea = newarea\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        crop = img[y:y+h,x:x+w]\n",
    "        return crop\n",
    "    \n",
    "    @classmethod\n",
    "    def crop_rect(cls, img):\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "        im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt = contours[0]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        cntarea = w*h\n",
    "        # not sure if already sorted so finding maximal area rectangle, that one's ours\n",
    "        for i in range(1, len(contours)):\n",
    "            x,y,w,h = cv2.boundingRect(contours[i])\n",
    "            newarea = w*h\n",
    "            if newarea > cntarea:\n",
    "                cnt = contours[i]\n",
    "                cntarea = newarea\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        crop = img[y:y+h,x:x+w]\n",
    "        return crop, (y, y+h, x, w+w)\n",
    "    \n",
    "    @classmethod\n",
    "    def mergeProto(cls, dst, src, threshold=10):\n",
    "        dstcpy = dst.copy()\n",
    "#         srccpy = src.copy()\n",
    "#         maskDst = (dstcpy[:,:, 0] < threshold) * (dstcpy[:,:, 1] < threshold) * (dstcpy[:,:, 2] < threshold)\n",
    "#         maskLeft = np.logical_not(maskDst)\n",
    "#         dstcpy[maskDst]=[0,0,0]\n",
    "#         srccpy[maskLeft]=[0,0,0]\n",
    "#         dstcpy += srccpy\n",
    "        # this gives me unspeakable pleasure\n",
    "        dstcpy[dstcpy < threshold] = src[dstcpy < threshold]\n",
    "        return dstcpy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoint detection in milliseconds:  19483.564000000002\n",
      "Matching in milliseconds:  1037.896\n",
      "Applying the transformation in milliseconds:  805.5989999999999\n",
      "Copying in milliseconds:  730.558\n",
      "Blending in milliseconds:  326.953\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "# pocetak = datetime.datetime.now()\n",
    "# print('asdasda')\n",
    "# kraj = datetime.datetime.now()\n",
    "# delta = kraj-pocetak\n",
    "# print(delta.total_seconds())\n",
    "right = cv2.imread('demo/yard-right.png')\n",
    "left = cv2.imread('demo/yard-left.png')\n",
    "# resizing, so it's faster\n",
    "#right = imutils.resize(right, width=600)\n",
    "#left = imutils.resize(left, width=600)\n",
    "#l, r = Panorama.padblack(left, right)\n",
    "res = Panorama.makePanorama(right, left, drawKeypoints=True, drawMatches=True, blend_type=Panorama.BOTH)\n",
    "#Panorama.crop_rect(r)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.imshow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('res2.png', res['resBlend'])\n",
    "#TODO time all the parts and try to speed this up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1200, 3)\n",
      "(400, 1200, 3)\n",
      "(400, 1200, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
