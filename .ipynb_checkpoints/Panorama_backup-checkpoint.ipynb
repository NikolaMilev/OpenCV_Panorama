{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#zero division error should be considered!\n",
    "# the idea roughly comes from here: http://bigwww.epfl.ch/publications/thevenaz0701.pdf\n",
    "# the above link found here: https://stackoverflow.com/questions/36386968/image-stitching-methods-to-remove-seams-for-stitched-image\n",
    "def blend(imgA, distA, imgB, distB, thresh=3):\n",
    "    result = np.zeros([imgA.shape[0], imgA.shape[1] ,3], dtype=np.uint8)\n",
    "    print(imgA.shape)\n",
    "    print(imgB.shape)\n",
    "    print(result.shape)\n",
    "    for i in range(imgA.shape[0]):\n",
    "        for j in range(imgA.shape[1]):\n",
    "            # if the current pixel is \"out of\" the first image (or is just black)\n",
    "            if np.all(imgA[i,j] < thresh):\n",
    "                result[i,j] = imgB[i,j]\n",
    "            # if the current pixel is \"out of\" the second image (or is just black)\n",
    "            elif np.all(imgB[i,j] < thresh):\n",
    "                result[i,j] = imgA[i,j]\n",
    "            # else, we make the weighted sum and do the magic\n",
    "            # notice that here distA[i,j]+distB[i,j] can't be zero, since\n",
    "            # the upper cases covered that\n",
    "            else:\n",
    "                result[i,j] = (imgA[i,j]*distA[i,j] + imgB[i,j]*distB[i,j]) / (distA[i,j]+distB[i,j])\n",
    "    return result\n",
    "\n",
    "def detectAndDescribe(image):\n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # detect and extract features from the image\n",
    "    descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "    (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "    return (kps, features)\n",
    "\n",
    "def drawKeypoints(image, keypoints):\n",
    "    kpsImg = None\n",
    "    kpsImg = cv2.drawKeypoints(image, keypoints, kpsImg)\n",
    "    return kpsImg\n",
    "\n",
    "right = cv2.imread('carmel-00.png')\n",
    "left = cv2.imread('carmel-01.png')\n",
    "right = imutils.resize(right, width=600)\n",
    "left = imutils.resize(left, width=600)\n",
    "(kpsR, ftsR) = detectAndDescribe(right)\n",
    "(kpsL, ftsL) = detectAndDescribe(left)\n",
    "#a = Stitcher.stitch([imageA, imageB], showMatches=True)\n",
    "# if a is not None:\n",
    "#     cv2.imwrite('out.png', a[0])\n",
    "#     cv2.imwrite('vis.png', a[1])\n",
    "kpsImgR = None\n",
    "kpsImgL = None\n",
    "kpsImgR = drawKeypoints(right, kpsR)\n",
    "kpsImgL = drawKeypoints(left, kpsL)\n",
    "cv2.imwrite('kpsR.png', kpsImgR)\n",
    "cv2.imwrite('kpsL.png', kpsImgL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orb = cv2.ORB_create()\n",
    "bf = cv2.BFMatcher(cv2.NORM_L1,crossCheck=False)\n",
    "matches = bf.match(ftsA,ftsB)\n",
    "matches = sorted(matches, key=lambda x:x.distance)\n",
    "# for m in matches[:10]:\n",
    "#     print(m.distance)\n",
    "matchImg = None\n",
    "matchImg = cv2.drawMatches(imageA, kpsA, imageB, kpsB, matches[:10], matchImg)\n",
    "cv2.imwrite('matched.png', matchImg)\n",
    "matchesIdx = [(m.queryIdx, m.trainIdx) for m in matches[:10]]\n",
    "matchesCoordA = np.float32([kpsA[mind[0]].pt for mind in matchesIdx])\n",
    "matchesCoordB = np.float32([kpsB[mind[1]].pt for mind in matchesIdx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(H, mask) = cv2.findHomography(matchesCoordR, matchesCoordL, cv2.RANSAC)\n",
    "#np.linalg.det(H[:2, :2])\n",
    "# B=cv2.getPerspectiveTransform(matchesCoordA[:4], matchesCoordB[:4])\n",
    "# np.linalg.det(B)\n",
    "imgDest = None\n",
    "imgDest = cv2.warpPerspective(right, H, (right.shape[1]+left.shape[1], left.shape[0]))\n",
    "alpha=0.5\n",
    "#imgDest=cv2.addWeighted(imgDest, alpha, newB, 1-alpha, 0)\n",
    "# blend jako lose radi, muti sliku, mora bolje\n",
    "#imgDest = blend(imgDest, imageB, 0.5)\n",
    "cv2.imwrite('warped.png', imgDest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "newL = np.zeros([left.shape[0], right.shape[1]+left.shape[1] ,3],dtype=np.uint8)\n",
    "newL[:] = [0, 0, 0]\n",
    "newL[0:left.shape[0], 0:left.shape[1]] = left\n",
    "newDst = imgDest\n",
    "newSrc = newL\n",
    "#cv2.imwrite('warped.png', imgDest)\n",
    "distDst = cv2.distanceTransform(cv2.cvtColor(newDst, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "distSrc = cv2.distanceTransform(cv2.cvtColor(newSrc, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "#cv2.imwrite('warped.png', distDst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zero division error should be considered!\n",
    "# the idea roughly comes from here: http://bigwww.epfl.ch/publications/thevenaz0701.pdf\n",
    "# the above link found here: https://stackoverflow.com/questions/36386968/image-stitching-methods-to-remove-seams-for-stitched-image\n",
    "def blend(imgA, distA, imgB, distB, thresh=3):\n",
    "    result = np.zeros([imgA.shape[0], imgA.shape[1] ,3], dtype=np.uint8)\n",
    "    print(imgA.shape)\n",
    "    print(imgB.shape)\n",
    "    print(result.shape)\n",
    "    for i in range(imgA.shape[0]):\n",
    "        for j in range(imgA.shape[1]):\n",
    "            # if the current pixel is \"out of\" the first image (or is just black)\n",
    "            if np.all(imgA[i,j] < thresh):\n",
    "                result[i,j] = imgB[i,j]\n",
    "            # if the current pixel is \"out of\" the second image (or is just black)\n",
    "            elif np.all(imgB[i,j] < thresh):\n",
    "                result[i,j] = imgA[i,j]\n",
    "            # else, we make the weighted sum and do the magic\n",
    "            # notice that here distA[i,j]+distB[i,j] can't be zero, since\n",
    "            # the upper cases covered that\n",
    "            else:\n",
    "                result[i,j] = (imgA[i,j]*distA[i,j] + imgB[i,j]*distB[i,j]) / (distA[i,j]+distB[i,j])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1200, 3)\n",
      "(400, 1200, 3)\n",
      "(400, 1200, 3)\n"
     ]
    }
   ],
   "source": [
    "result=blend1(newDst, distDst, newSrc, distSrc)\n",
    "cv2.imwrite('warped.png', result)\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
