{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "\n",
    "class Panorama:\n",
    "\n",
    "    # the blend types\n",
    "    NO_BLEND = 1\n",
    "    BLEND_WEIGHED  = 2\n",
    "    BOTH = 3\n",
    "    \n",
    "    # we make the weighed average of the two images\n",
    "    # the idea roughly comes from here: http://bigwww.epfl.ch/publications/thevenaz0701.pdf\n",
    "    # the above link found here: https://stackoverflow.com/questions/36386968/image-stitching-methods-to-remove-seams-for-stitched-image\n",
    "    @classmethod\n",
    "    def blendWeighed(cls, imgA, imgB, thresh=3):\n",
    "        result = np.zeros([imgA.shape[0], imgA.shape[1] ,3], dtype=np.uint8)\n",
    "        distA = cv2.distanceTransform(cv2.cvtColor(imgA, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "        distB = cv2.distanceTransform(cv2.cvtColor(imgB, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "        #cv2.imwrite('distA.png', distA)\n",
    "        #cv2.imwrite('distB.png', distB)\n",
    "        # for every pixel within the result image (that has the same dimensions as the imgA/imgB!)\n",
    "        for i in range(imgA.shape[0]):\n",
    "            for j in range(imgA.shape[1]):\n",
    "                # if the current pixel is \"out of\" the first image (or is just black)\n",
    "                if np.all(imgA[i,j] < thresh):\n",
    "                    result[i,j] = imgB[i,j]\n",
    "                # if the current pixel is \"out of\" the second image (or is just black)\n",
    "                elif np.all(imgB[i,j] < thresh):\n",
    "                    result[i,j] = imgA[i,j]\n",
    "                # else, we make the weighted sum and do the magic\n",
    "                # notice that here distA[i,j]+distB[i,j] can't be zero, since\n",
    "                # the upper cases covered that\n",
    "                else:\n",
    "                    result[i,j] = (imgA[i,j]*distA[i,j] + imgB[i,j]*distB[i,j]) / (distA[i,j]+distB[i,j])\n",
    "        \n",
    "        \n",
    "        return result\n",
    "   \n",
    "    # naive merge; seams are to be seen, probably\n",
    "    @classmethod\n",
    "    def merge(cls, imgDest, imgSrc):\n",
    "        # and just paste the second image onto the first one\n",
    "        result = np.copy(imgDest)\n",
    "        result[0:imgSrc.shape[0], 0:imgSrc.shape[1]] = imgSrc\n",
    "        return result\n",
    "    \n",
    "    # obtaining the keypoints and their features for an image\n",
    "    @classmethod\n",
    "    def detectAndDescribe(cls, image):\n",
    "        # convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and extract features from the image\n",
    "        descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "        (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "        return (kps, features)\n",
    "    \n",
    "    # the method for drawing matches in the image left and right, given the \n",
    "    # keypoints and matches\n",
    "    @classmethod\n",
    "    def drawMatches(cls, right, kpsR, left, kpsL, matches):\n",
    "        matchImg = None\n",
    "        matchImg = cv2.drawMatches(right, kpsR, left, kpsL, matches, matchImg)\n",
    "        return matchImg\n",
    "    @classmethod\n",
    "    def drawKeypoints(cls, image, keypoints):\n",
    "        draw = None\n",
    "        draw = cv2.drawKeypoints(image, keypoints, draw)\n",
    "        return draw\n",
    "    \n",
    "    @classmethod\n",
    "    def transform(cls, matches, kpsR, kpsL):\n",
    "        matchesIdx = [(m.queryIdx, m.trainIdx) for m in matches[:10]]\n",
    "        matchesCoordR = np.float32([kpsR[mind[0]].pt for mind in matchesIdx])\n",
    "        matchesCoordL = np.float32([kpsL[mind[1]].pt for mind in matchesIdx])\n",
    "        (H, mask) = cv2.findHomography(matchesCoordR, matchesCoordL, cv2.RANSAC)\n",
    "        \n",
    "        imgDest = None\n",
    "        imgDest = cv2.warpPerspective(right, H, (right.shape[1]+left.shape[1], left.shape[0] + right.shape[0]))\n",
    "        \n",
    "        return imgDest\n",
    "        \n",
    "    @classmethod\n",
    "    def makePanorama(cls, right, left, drawKeypoints=False, drawMatches=False, blend_type=NO_BLEND):\n",
    "        # the result is a dictionary that contains all the possible images derived:\n",
    "        # result with no blending\n",
    "        # result with blending\n",
    "        # right image with keypoints drawn\n",
    "        # left image with keypoints drawn\n",
    "        # both images with matches drawn\n",
    "        result = {'resNoBlend':None, 'resBlend':None, 'kpsR':None, 'kpsL':None, 'matches':None}\n",
    "        \n",
    "        # first, we find the keypoints in the images and their features \n",
    "        (kpsR, ftsR) = cls.detectAndDescribe(right)\n",
    "        (kpsL, ftsL) = cls.detectAndDescribe(left)\n",
    "        \n",
    "        # if we should draw the keypoints, we draw them and put in the dictionary\n",
    "        if drawKeypoints:\n",
    "            result['kpsR'] = cls.drawKeypoints(right, kpsR)\n",
    "            result['kpsL'] = cls.drawKeypoints(left, kpsL)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #orb = cv2.ORB_create()\n",
    "        \n",
    "        # we find the matcher for the images and sort them by distance (by relevance)\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L1,crossCheck=False)\n",
    "        matches = bf.match(ftsR,ftsL)\n",
    "        matches = sorted(matches, key=lambda x:x.distance)\n",
    "        \n",
    "        # if we should draw the matches, we draw them and put the result in the dictionary\n",
    "        if drawMatches:\n",
    "            result['matches'] = cls.drawMatches(right, kpsR, left, kpsL, matches[:10])\n",
    "        \n",
    "        # we transform the right image\n",
    "        newDst = cls.transform(matches, kpsR, kpsL)\n",
    "        #cv2.imwrite('newDst.png', Panorama.crop(newDst))\n",
    "       \n",
    "        # we pad the left image with black background so that the two images have matching dimensions\n",
    "        newSrc = np.zeros([left.shape[0] + right.shape[0], right.shape[1]+left.shape[1] ,3],dtype=np.uint8)\n",
    "        newSrc[0:left.shape[0], 0:left.shape[1]] = left\n",
    "        #cv2.imwrite('newSrc.png', newSrc)\n",
    "        \n",
    "        # now, both images that we use are in newDst (right) and newSrc (left)\n",
    "        \n",
    "        # depending on the type of blending chosen, we blend (or don't) and put the results \n",
    "        # in the dictionary\n",
    "        if blend_type == cls.NO_BLEND or blend_type == cls.BOTH:\n",
    "            # since the newSrc is now padded, we use left!\n",
    "            result['resNoBlend'] = cls.merge(newDst, left)\n",
    "        if blend_type == cls.BLEND_WEIGHED or blend_type == cls.BOTH:\n",
    "            result['resBlend'] = cls.blendWeighed(newDst, newSrc)\n",
    "        \n",
    "        return result\n",
    "    # cropping doesn't remove ALL the black parts, just the maximal amount that\n",
    "    # keeps all the pixels of the stitched images \n",
    "    @classmethod\n",
    "    def crop(cls,img):\n",
    "        imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret, thresh = cv2.threshold(imgray, 10, 10, 10)\n",
    "        im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        maxa = 0\n",
    "        maxi = 0\n",
    "        for i in range(len(contours)):\n",
    "            tmpmax = cv2.contourArea(contours[i])\n",
    "            if maxa < tmpmax:\n",
    "                maxa = tmpmax\n",
    "                maxi = i\n",
    "\n",
    "        rect=cv2.minAreaRect(contours[maxi])\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "\n",
    "        x = box[1][0]\n",
    "        y = box[2][1]\n",
    "        w = box[0][0] - box[1][0]\n",
    "        h = box[1][1] - box[2][1]\n",
    "        result = img[y: y + h, x: x + w]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "right = cv2.imread('demo/yard-right.png')\n",
    "left = cv2.imread('demo/yard-left.png')\n",
    "# resizing, so it's faster\n",
    "#right = imutils.resize(right, width=600)\n",
    "#left = imutils.resize(left, width=600)\n",
    "res = Panorama.makePanorama(right, left, drawKeypoints=True, drawMatches=True, blend_type=Panorama.BOTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('demo/res1.png', Panorama.crop(res['resBlend']))\n",
    "cv2.imwrite('demo/res2.png', Panorama.crop(res['resNoBlend']))\n",
    "cv2.imwrite('demo/matches.png', res['matches'])\n",
    "cv2.imwrite('demo/kpsL.png', res['kpsL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
