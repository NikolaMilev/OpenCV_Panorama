{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import datetime\n",
    "\n",
    "class Panorama:\n",
    "\n",
    "    # the blend types\n",
    "    NO_BLEND = 1\n",
    "    BOTH = 2\n",
    "    \n",
    "    # padding the images so we don't lose any information\n",
    "    @classmethod\n",
    "    def padblack(cls, left, right, increase=2):\n",
    "        h = (left.shape[0] + right.shape[0])*increase\n",
    "        w = (right.shape[1]+left.shape[1])*increase\n",
    "        leftpadded = np.zeros([h, w ,3],dtype=np.uint8)\n",
    "        rightpadded = np.zeros([h, w ,3],dtype=np.uint8)\n",
    "        leftbeg = (w-left.shape[1])//2\n",
    "        upbeg = (h-left.shape[0])//2\n",
    "        leftpadded[upbeg:upbeg+left.shape[0], leftbeg:leftbeg+left.shape[1]] = left\n",
    "        rightpadded[upbeg:upbeg+right.shape[0], leftbeg:leftbeg+right.shape[1]] = right\n",
    "        return (leftpadded, rightpadded)\n",
    "        \n",
    "    # the new blending\n",
    "    # using slicing instead of for loops, it's much faster\n",
    "    # we make the weighed average of the two images\n",
    "    # the idea roughly comes from here: http://bigwww.epfl.ch/publications/thevenaz0701.pdf\n",
    "    # the above link found here: https://stackoverflow.com/questions/36386968/image-stitching-methods-to-remove-seams-for-stitched-image\n",
    "    @classmethod\n",
    "    def blendWeighed(cls, imgA, imgB, threshold=3):\n",
    "        distA = cv2.distanceTransform(cv2.cvtColor(imgA, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "        distB = cv2.distanceTransform(cv2.cvtColor(imgB, cv2.COLOR_BGR2GRAY), cv2.DIST_L2, 5)\n",
    "        result = np.zeros(imgA.shape)\n",
    "        # I don't know better atm but the dimensions of the distA, distB and the imgA and imgB are not fitting\n",
    "        # and I cannot use slicing if they don't fit\n",
    "        a = np.asarray(np.dstack((distA, distA, distA)), dtype=np.float32)\n",
    "        b = np.asarray(np.dstack((distB, distB, distB)), dtype=np.float32)\n",
    "        div = a + b\n",
    "        mask = (imgA >= threshold) & (imgB >= threshold)\n",
    "        result[mask] = (a*imgA+b*imgB)[mask]/div[mask]\n",
    "        result[imgA < threshold] = imgB[imgA < threshold]\n",
    "        result[imgB < threshold] = imgA[imgB < threshold]\n",
    "        return result\n",
    "    \n",
    "    # obtaining the keypoints and their features for an image\n",
    "    @classmethod\n",
    "    def detectAndDescribe(cls, image):\n",
    "        # convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and extract features from the image\n",
    "        descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "        (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "        return (kps, features)\n",
    "    \n",
    "    # the method for drawing matches in the image left and right, given the \n",
    "    # keypoints and matches\n",
    "    @classmethod\n",
    "    def drawMatches(cls, right, kpsR, left, kpsL, matches):\n",
    "        matchImg = None\n",
    "        matchImg = cv2.drawMatches(right, kpsR, left, kpsL, matches, matchImg)\n",
    "        return matchImg\n",
    "    @classmethod\n",
    "    def drawKeypoints(cls, image, keypoints):\n",
    "        draw = None\n",
    "        draw = cv2.drawKeypoints(image, keypoints, draw)\n",
    "        return draw\n",
    "    \n",
    "    @classmethod\n",
    "    def transform(cls, matches, kpsR, kpsL, left, right):\n",
    "        matchesIdx = [(m.queryIdx, m.trainIdx) for m in matches[:10]]\n",
    "        matchesCoordR = np.float32([kpsR[mind[0]].pt for mind in matchesIdx])\n",
    "        matchesCoordL = np.float32([kpsL[mind[1]].pt for mind in matchesIdx])\n",
    "        (H, mask) = cv2.findHomography(matchesCoordR, matchesCoordL, cv2.RANSAC)\n",
    "        \n",
    "        imgDest = None\n",
    "        imgDest = cv2.warpPerspective(right, H, (right.shape[1], right.shape[0]))\n",
    "        \n",
    "        return imgDest\n",
    "        \n",
    "    @classmethod\n",
    "    def makePanorama(cls, right, left, drawKeypoints=False, drawMatches=False, blend_type=NO_BLEND):\n",
    "        # the result is a dictionary that contains all the possible images derived:\n",
    "        # result with no blending\n",
    "        # result with blending\n",
    "        # right image with keypoints drawn\n",
    "        # left image with keypoints drawn\n",
    "        # both images with matches drawn\n",
    "        result = {'resNoBlend':None, 'resBlend':None, 'kpsR':None, 'kpsL':None, 'matches':None}\n",
    "        beg = datetime.datetime.now()\n",
    "        \n",
    "        l, r = cls.padblack(left, right)\n",
    "        left, right = l, r\n",
    "        \n",
    "        # first, we find the keypoints in the images and their features \n",
    "        (kpsR, ftsR) = cls.detectAndDescribe(right)\n",
    "        (kpsL, ftsL) = cls.detectAndDescribe(left)\n",
    "        end = datetime.datetime.now()\n",
    "        print(\"Keypoint detection in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "        # if we should draw the keypoints, we draw them and put in the dictionary\n",
    "        if drawKeypoints:\n",
    "            result['kpsR'] = cls.drawKeypoints(right, kpsR)\n",
    "            result['kpsL'] = cls.drawKeypoints(left, kpsL)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #orb = cv2.ORB_create()\n",
    "        \n",
    "        # we find the matcher for the images and sort them by distance (by relevance)\n",
    "        beg = datetime.datetime.now()\n",
    "        \n",
    "        bf = cv2.BFMatcher(cv2.NORM_L1,crossCheck=False)\n",
    "        matches = bf.match(ftsR,ftsL)\n",
    "        matches = sorted(matches, key=lambda x:x.distance)\n",
    "        \n",
    "        end = datetime.datetime.now()\n",
    "        print(\"Matching in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "        \n",
    "        # if we should draw the matches, we draw them and put the result in the dictionary\n",
    "        if drawMatches:\n",
    "            result['matches'] = cls.drawMatches(right, kpsR, left, kpsL, matches[:10])\n",
    "        \n",
    "        beg = datetime.datetime.now()\n",
    "        \n",
    "        # we transform the right image\n",
    "        newDst = cls.transform(matches, kpsR, kpsL, left, right)\n",
    "        #cv2.imwrite('newDst.png', newDst)\n",
    "       \n",
    "        end = datetime.datetime.now()\n",
    "        print(\"Applying the transformation in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "    \n",
    "        # we pad the left image with black background so that the two images have matching dimensions\n",
    "        #newSrc = np.zeros([left.shape[0] + right.shape[0], right.shape[1]+left.shape[1] ,3],dtype=np.uint8)\n",
    "        #newSrc[0:left.shape[0], 0:left.shape[1]] = left\n",
    "        newSrc = left\n",
    "        #cv2.imwrite('newSrc.png', newSrc)\n",
    "        \n",
    "        # now, both images that we use are in newDst (right) and newSrc (left)\n",
    "        \n",
    "        # depending on the type of blending chosen, we blend (or don't) and put the results \n",
    "        # in the dictionary\n",
    "        # since the newSrc is now padded, we use left!\n",
    "        beg = datetime.datetime.now()\n",
    "        \n",
    "        tmp_no_blend = cls.mergeProto(newDst, newSrc)\n",
    "        cropped, (y, yh, x, xw) = cls.crop_rect(tmp_no_blend)\n",
    "        result['resNoBlend'] = cropped\n",
    "        newDst1 = newDst[y:yh, x:xw]\n",
    "        newSrc1 = newSrc[y:yh, x:xw]\n",
    "    \n",
    "        end = datetime.datetime.now()\n",
    "        print(\"Copying in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "        \n",
    "        if blend_type == cls.BOTH:\n",
    "            beg = datetime.datetime.now()\n",
    "            \n",
    "            result['resBlend'] = cls.blendWeighed(newDst1, newSrc1)\n",
    "            \n",
    "            end = datetime.datetime.now()\n",
    "            print(\"Blending in milliseconds: \", (end-beg).total_seconds()*1000)\n",
    "        \n",
    "        return result\n",
    "    # cropping doesn't remove ALL the black parts, just the maximal amount that\n",
    "    # keeps all the pixels of the stitched images \n",
    "    @classmethod\n",
    "    def crop_new(cls,img):\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "        im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt = contours[0]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        cntarea = w*h\n",
    "        # not sure if already sorted so finding maximal area rectangle, that one's ours\n",
    "        for i in range(1, len(contours)):\n",
    "            x,y,w,h = cv2.boundingRect(contours[i])\n",
    "            newarea = w*h\n",
    "            if newarea > cntarea:\n",
    "                cnt = contours[i]\n",
    "                cntarea = newarea\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        crop = img[y:y+h,x:x+w]\n",
    "        return crop\n",
    "    \n",
    "    @classmethod\n",
    "    def crop_rect(cls, img):\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "        im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt = contours[0]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        cntarea = w*h\n",
    "        # not sure if already sorted so finding maximal area rectangle, that one's ours\n",
    "        for i in range(1, len(contours)):\n",
    "            x,y,w,h = cv2.boundingRect(contours[i])\n",
    "            newarea = w*h\n",
    "            if newarea > cntarea:\n",
    "                cnt = contours[i]\n",
    "                cntarea = newarea\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        crop = img[y:y+h,x:x+w]\n",
    "        return crop, (y, y+h, x, x+w)\n",
    "    \n",
    "    @classmethod\n",
    "    def mergeProto(cls, im1, im2, threshold=10):\n",
    "        im2cpy = im2.copy()\n",
    "        # this gives me unspeakable pleasure\n",
    "        mask = (im2cpy < threshold) & (im1 >= threshold)\n",
    "        im2cpy[mask] = im1[mask]\n",
    "        return im2cpy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoint detection in milliseconds:  9174.051000000001\n",
      "Matching in milliseconds:  1200.494\n",
      "Applying the transformation in milliseconds:  71.291\n",
      "Copying in milliseconds:  117.982\n",
      "Blending in milliseconds:  95.165\n",
      "Total time:  10.832165 s\n"
     ]
    }
   ],
   "source": [
    "beg = datetime.datetime.now()\n",
    "\n",
    "right = cv2.imread('carmel-00.png')\n",
    "left = cv2.imread('carmel-01.png')\n",
    "# resizing, so it's faster\n",
    "#right = imutils.resize(right, width=600)\n",
    "#left = imutils.resize(left, width=600)\n",
    "#l, r = Panorama.padblack(left, right)\n",
    "res = Panorama.makePanorama(right, left, drawKeypoints=True, drawMatches=True, blend_type=Panorama.BOTH)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print(\"Total time: \", (end-beg).total_seconds(), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('demo/resBlendCarmel.png', res['resBlend'])\n",
    "cv2.imwrite('demo/resNoBlendCarmel.png', res['resNoBlend'])\n",
    "#TODO time all the parts and try to speed this up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
